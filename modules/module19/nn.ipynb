{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.043580 -0.005970 -0.035054 -0.995381 -0.988366 -0.937382 -0.995007   \n",
       "1     0.039480 -0.002131 -0.029067 -0.998348 -0.982945 -0.971273 -0.998702   \n",
       "2     0.039978 -0.005153 -0.022651 -0.995482 -0.977314 -0.984760 -0.996415   \n",
       "3     0.039785 -0.011809 -0.028916 -0.996194 -0.988569 -0.993256 -0.996994   \n",
       "4     0.038758 -0.002289 -0.023863 -0.998241 -0.986774 -0.993115 -0.998216   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7762  0.048048 -0.042445 -0.065884 -0.195448 -0.278326 -0.219954 -0.282233   \n",
       "7763  0.037639  0.006430 -0.044345 -0.235372 -0.302680 -0.232843 -0.322483   \n",
       "7764  0.037451 -0.002724  0.021009 -0.218281 -0.378082 -0.076950 -0.304446   \n",
       "7765  0.044011 -0.004536 -0.051242 -0.219202 -0.383350 -0.081035 -0.310419   \n",
       "7766  0.068954  0.001810 -0.080323 -0.269336 -0.366553 -0.147294 -0.377332   \n",
       "\n",
       "           7         8         9    ...       551       552       553  \\\n",
       "0    -0.988816 -0.953325 -0.794796  ... -0.012236 -0.314848 -0.713308   \n",
       "1    -0.983315 -0.974000 -0.802537  ...  0.202804 -0.603199 -0.860677   \n",
       "2    -0.975835 -0.985973 -0.798477  ...  0.440079 -0.404427 -0.761847   \n",
       "3    -0.988526 -0.993135 -0.798477  ...  0.430891 -0.138373 -0.491604   \n",
       "4    -0.986479 -0.993825 -0.801982  ...  0.137735 -0.366214 -0.702490   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7762 -0.305861 -0.357803  0.267874  ... -0.008381 -0.596760 -0.879026   \n",
       "7763 -0.354464 -0.345592  0.181271  ...  0.209452 -0.404418 -0.684496   \n",
       "7764 -0.400661 -0.193071  0.113141  ...  0.237003  0.000207 -0.317314   \n",
       "7765 -0.380233 -0.201007  0.166671  ...  0.069366  0.037919 -0.356579   \n",
       "7766 -0.360597 -0.255505  0.321881  ...  0.002496 -0.400831 -0.742972   \n",
       "\n",
       "           554       555       556       557       558       559       560  \n",
       "0    -0.112754  0.030400 -0.464761 -0.018446 -0.841559  0.179913 -0.051718  \n",
       "1     0.053477 -0.007435 -0.732626  0.703511 -0.845092  0.180261 -0.047436  \n",
       "2    -0.118559  0.177899  0.100699  0.808529 -0.849230  0.180610 -0.042271  \n",
       "3    -0.036788 -0.012892  0.640011 -0.485366 -0.848947  0.181907 -0.040826  \n",
       "4     0.123320  0.122542  0.693578 -0.615971 -0.848164  0.185124 -0.037080  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7762 -0.190437  0.829718  0.206972 -0.425619 -0.792292  0.238580  0.056020  \n",
       "7763  0.064907  0.875679 -0.879033  0.400219 -0.772288  0.252653  0.056252  \n",
       "7764  0.052806 -0.266724  0.864404  0.701169 -0.779566  0.249121  0.047071  \n",
       "7765 -0.101360  0.700740  0.936674 -0.589479 -0.785603  0.246409  0.031700  \n",
       "7766 -0.280088 -0.007739 -0.056088 -0.616956 -0.783693  0.246785  0.042981  \n",
       "\n",
       "[7767 rows x 561 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>551</th>\n      <th>552</th>\n      <th>553</th>\n      <th>554</th>\n      <th>555</th>\n      <th>556</th>\n      <th>557</th>\n      <th>558</th>\n      <th>559</th>\n      <th>560</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.043580</td>\n      <td>-0.005970</td>\n      <td>-0.035054</td>\n      <td>-0.995381</td>\n      <td>-0.988366</td>\n      <td>-0.937382</td>\n      <td>-0.995007</td>\n      <td>-0.988816</td>\n      <td>-0.953325</td>\n      <td>-0.794796</td>\n      <td>...</td>\n      <td>-0.012236</td>\n      <td>-0.314848</td>\n      <td>-0.713308</td>\n      <td>-0.112754</td>\n      <td>0.030400</td>\n      <td>-0.464761</td>\n      <td>-0.018446</td>\n      <td>-0.841559</td>\n      <td>0.179913</td>\n      <td>-0.051718</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.039480</td>\n      <td>-0.002131</td>\n      <td>-0.029067</td>\n      <td>-0.998348</td>\n      <td>-0.982945</td>\n      <td>-0.971273</td>\n      <td>-0.998702</td>\n      <td>-0.983315</td>\n      <td>-0.974000</td>\n      <td>-0.802537</td>\n      <td>...</td>\n      <td>0.202804</td>\n      <td>-0.603199</td>\n      <td>-0.860677</td>\n      <td>0.053477</td>\n      <td>-0.007435</td>\n      <td>-0.732626</td>\n      <td>0.703511</td>\n      <td>-0.845092</td>\n      <td>0.180261</td>\n      <td>-0.047436</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.039978</td>\n      <td>-0.005153</td>\n      <td>-0.022651</td>\n      <td>-0.995482</td>\n      <td>-0.977314</td>\n      <td>-0.984760</td>\n      <td>-0.996415</td>\n      <td>-0.975835</td>\n      <td>-0.985973</td>\n      <td>-0.798477</td>\n      <td>...</td>\n      <td>0.440079</td>\n      <td>-0.404427</td>\n      <td>-0.761847</td>\n      <td>-0.118559</td>\n      <td>0.177899</td>\n      <td>0.100699</td>\n      <td>0.808529</td>\n      <td>-0.849230</td>\n      <td>0.180610</td>\n      <td>-0.042271</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.039785</td>\n      <td>-0.011809</td>\n      <td>-0.028916</td>\n      <td>-0.996194</td>\n      <td>-0.988569</td>\n      <td>-0.993256</td>\n      <td>-0.996994</td>\n      <td>-0.988526</td>\n      <td>-0.993135</td>\n      <td>-0.798477</td>\n      <td>...</td>\n      <td>0.430891</td>\n      <td>-0.138373</td>\n      <td>-0.491604</td>\n      <td>-0.036788</td>\n      <td>-0.012892</td>\n      <td>0.640011</td>\n      <td>-0.485366</td>\n      <td>-0.848947</td>\n      <td>0.181907</td>\n      <td>-0.040826</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.038758</td>\n      <td>-0.002289</td>\n      <td>-0.023863</td>\n      <td>-0.998241</td>\n      <td>-0.986774</td>\n      <td>-0.993115</td>\n      <td>-0.998216</td>\n      <td>-0.986479</td>\n      <td>-0.993825</td>\n      <td>-0.801982</td>\n      <td>...</td>\n      <td>0.137735</td>\n      <td>-0.366214</td>\n      <td>-0.702490</td>\n      <td>0.123320</td>\n      <td>0.122542</td>\n      <td>0.693578</td>\n      <td>-0.615971</td>\n      <td>-0.848164</td>\n      <td>0.185124</td>\n      <td>-0.037080</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7762</th>\n      <td>0.048048</td>\n      <td>-0.042445</td>\n      <td>-0.065884</td>\n      <td>-0.195448</td>\n      <td>-0.278326</td>\n      <td>-0.219954</td>\n      <td>-0.282233</td>\n      <td>-0.305861</td>\n      <td>-0.357803</td>\n      <td>0.267874</td>\n      <td>...</td>\n      <td>-0.008381</td>\n      <td>-0.596760</td>\n      <td>-0.879026</td>\n      <td>-0.190437</td>\n      <td>0.829718</td>\n      <td>0.206972</td>\n      <td>-0.425619</td>\n      <td>-0.792292</td>\n      <td>0.238580</td>\n      <td>0.056020</td>\n    </tr>\n    <tr>\n      <th>7763</th>\n      <td>0.037639</td>\n      <td>0.006430</td>\n      <td>-0.044345</td>\n      <td>-0.235372</td>\n      <td>-0.302680</td>\n      <td>-0.232843</td>\n      <td>-0.322483</td>\n      <td>-0.354464</td>\n      <td>-0.345592</td>\n      <td>0.181271</td>\n      <td>...</td>\n      <td>0.209452</td>\n      <td>-0.404418</td>\n      <td>-0.684496</td>\n      <td>0.064907</td>\n      <td>0.875679</td>\n      <td>-0.879033</td>\n      <td>0.400219</td>\n      <td>-0.772288</td>\n      <td>0.252653</td>\n      <td>0.056252</td>\n    </tr>\n    <tr>\n      <th>7764</th>\n      <td>0.037451</td>\n      <td>-0.002724</td>\n      <td>0.021009</td>\n      <td>-0.218281</td>\n      <td>-0.378082</td>\n      <td>-0.076950</td>\n      <td>-0.304446</td>\n      <td>-0.400661</td>\n      <td>-0.193071</td>\n      <td>0.113141</td>\n      <td>...</td>\n      <td>0.237003</td>\n      <td>0.000207</td>\n      <td>-0.317314</td>\n      <td>0.052806</td>\n      <td>-0.266724</td>\n      <td>0.864404</td>\n      <td>0.701169</td>\n      <td>-0.779566</td>\n      <td>0.249121</td>\n      <td>0.047071</td>\n    </tr>\n    <tr>\n      <th>7765</th>\n      <td>0.044011</td>\n      <td>-0.004536</td>\n      <td>-0.051242</td>\n      <td>-0.219202</td>\n      <td>-0.383350</td>\n      <td>-0.081035</td>\n      <td>-0.310419</td>\n      <td>-0.380233</td>\n      <td>-0.201007</td>\n      <td>0.166671</td>\n      <td>...</td>\n      <td>0.069366</td>\n      <td>0.037919</td>\n      <td>-0.356579</td>\n      <td>-0.101360</td>\n      <td>0.700740</td>\n      <td>0.936674</td>\n      <td>-0.589479</td>\n      <td>-0.785603</td>\n      <td>0.246409</td>\n      <td>0.031700</td>\n    </tr>\n    <tr>\n      <th>7766</th>\n      <td>0.068954</td>\n      <td>0.001810</td>\n      <td>-0.080323</td>\n      <td>-0.269336</td>\n      <td>-0.366553</td>\n      <td>-0.147294</td>\n      <td>-0.377332</td>\n      <td>-0.360597</td>\n      <td>-0.255505</td>\n      <td>0.321881</td>\n      <td>...</td>\n      <td>0.002496</td>\n      <td>-0.400831</td>\n      <td>-0.742972</td>\n      <td>-0.280088</td>\n      <td>-0.007739</td>\n      <td>-0.056088</td>\n      <td>-0.616956</td>\n      <td>-0.783693</td>\n      <td>0.246785</td>\n      <td>0.042981</td>\n    </tr>\n  </tbody>\n</table>\n<p>7767 rows Ã— 561 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Read the input features into `X`\n",
    "X = pd.read_csv(Path(\"./Resources/features.csv\"), header=None)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   activity\n",
       "0  standing\n",
       "1  standing\n",
       "2  standing\n",
       "3  standing\n",
       "4  standing"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>activity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>standing</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>standing</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>standing</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>standing</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>standing</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Read the target values into `y`\n",
    "y = pd.read_csv(Path(\"./Resources/target.csv\"))\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "standing              1423\n",
       "laying                1413\n",
       "sitting               1293\n",
       "walking               1226\n",
       "walking_upstairs      1073\n",
       "walking_downstairs     987\n",
       "stand_to_lie            90\n",
       "sit_to_lie              75\n",
       "lie_to_sit              60\n",
       "lie_to_stand            57\n",
       "stand_to_sit            47\n",
       "sit_to_stand            23\n",
       "Name: activity, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "y.activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Split the dataset into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "# Scale the training and testing input features using StandardScaler\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "# Apply One-hot encoding to the target labels\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train)\n",
    "encoded_y_train = enc.transform(y_train).toarray()\n",
    "encoded_y_test = enc.transform(y_test).toarray()\n",
    "encoded_y_train[0]"
   ]
  },
  {
   "source": [
    "# Build NN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first layer where the input dimensions are the 561 columns of the training data\n",
    "model.add(Dense(100, activation='relu', input_dim=X_train_scaled.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "X_train_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "laying                1058\nstanding              1056\nsitting                956\nwalking                926\nwalking_upstairs       806\nwalking_downstairs     757\nstand_to_lie            72\nsit_to_lie              52\nlie_to_stand            46\nlie_to_sit              45\nstand_to_sit            32\nsit_to_stand            19\nName: activity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The output layer has 12 columns that are one-hot encoded\n",
    "print(y_train.activity.value_counts())\n",
    "number_outputs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layer using 12 output nodes\n",
    "model.add(Dense(number_outputs, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using categorical_crossentropy for the loss function, the adam optimizer,\n",
    "# and add accuracy to the training metrics\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 100)               56200     \n_________________________________________________________________\ndense_1 (Dense)              (None, 12)                1212      \n_________________________________________________________________\ndense_2 (Dense)              (None, 12)                156       \n=================================================================\nTotal params: 57,568\nTrainable params: 57,568\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 5825 samples\n",
      "Epoch 1/100\n",
      "5825/5825 - 1s - loss: 0.6442 - accuracy: 0.6992\n",
      "Epoch 2/100\n",
      "5825/5825 - 1s - loss: 0.6404 - accuracy: 0.6987\n",
      "Epoch 3/100\n",
      "5825/5825 - 1s - loss: 0.6304 - accuracy: 0.7011\n",
      "Epoch 4/100\n",
      "5825/5825 - 1s - loss: 0.6252 - accuracy: 0.7011\n",
      "Epoch 5/100\n",
      "5825/5825 - 1s - loss: 0.6192 - accuracy: 0.7016\n",
      "Epoch 6/100\n",
      "5825/5825 - 1s - loss: 0.6149 - accuracy: 0.7016\n",
      "Epoch 7/100\n",
      "5825/5825 - 1s - loss: 0.6110 - accuracy: 0.7015\n",
      "Epoch 8/100\n",
      "5825/5825 - 1s - loss: 0.6054 - accuracy: 0.7025\n",
      "Epoch 9/100\n",
      "5825/5825 - 1s - loss: 0.6022 - accuracy: 0.7013\n",
      "Epoch 10/100\n",
      "5825/5825 - 1s - loss: 0.5997 - accuracy: 0.7013\n",
      "Epoch 11/100\n",
      "5825/5825 - 1s - loss: 0.6147 - accuracy: 0.6967\n",
      "Epoch 12/100\n",
      "5825/5825 - 1s - loss: 0.5982 - accuracy: 0.7004\n",
      "Epoch 13/100\n",
      "5825/5825 - 1s - loss: 0.5905 - accuracy: 0.7018\n",
      "Epoch 14/100\n",
      "5825/5825 - 1s - loss: 0.5904 - accuracy: 0.7013\n",
      "Epoch 15/100\n",
      "5825/5825 - 1s - loss: 0.5837 - accuracy: 0.7013\n",
      "Epoch 16/100\n",
      "5825/5825 - 1s - loss: 0.5833 - accuracy: 0.7011\n",
      "Epoch 17/100\n",
      "5825/5825 - 1s - loss: 0.5902 - accuracy: 0.6953\n",
      "Epoch 18/100\n",
      "5825/5825 - 1s - loss: 0.5797 - accuracy: 0.7004\n",
      "Epoch 19/100\n",
      "5825/5825 - 1s - loss: 0.5749 - accuracy: 0.7015\n",
      "Epoch 20/100\n",
      "5825/5825 - 1s - loss: 0.5712 - accuracy: 0.7021\n",
      "Epoch 21/100\n",
      "5825/5825 - 1s - loss: 0.5692 - accuracy: 0.7020\n",
      "Epoch 22/100\n",
      "5825/5825 - 1s - loss: 0.5661 - accuracy: 0.7025\n",
      "Epoch 23/100\n",
      "5825/5825 - 1s - loss: 0.5720 - accuracy: 0.6991\n",
      "Epoch 24/100\n",
      "5825/5825 - 1s - loss: 0.5665 - accuracy: 0.7011\n",
      "Epoch 25/100\n",
      "5825/5825 - 1s - loss: 0.5604 - accuracy: 0.7030\n",
      "Epoch 26/100\n",
      "5825/5825 - 1s - loss: 0.5577 - accuracy: 0.7033\n",
      "Epoch 27/100\n",
      "5825/5825 - 1s - loss: 0.5617 - accuracy: 0.6999\n",
      "Epoch 28/100\n",
      "5825/5825 - 1s - loss: 0.5604 - accuracy: 0.7008\n",
      "Epoch 29/100\n",
      "5825/5825 - 1s - loss: 0.5583 - accuracy: 0.7011\n",
      "Epoch 30/100\n",
      "5825/5825 - 1s - loss: 0.5519 - accuracy: 0.7037\n",
      "Epoch 31/100\n",
      "5825/5825 - 1s - loss: 0.5511 - accuracy: 0.7033\n",
      "Epoch 32/100\n",
      "5825/5825 - 1s - loss: 0.5490 - accuracy: 0.7037\n",
      "Epoch 33/100\n",
      "5825/5825 - 1s - loss: 0.5475 - accuracy: 0.7039\n",
      "Epoch 34/100\n",
      "5825/5825 - 1s - loss: 0.5463 - accuracy: 0.7039\n",
      "Epoch 35/100\n",
      "5825/5825 - 1s - loss: 0.5453 - accuracy: 0.7039\n",
      "Epoch 36/100\n",
      "5825/5825 - 1s - loss: 0.5444 - accuracy: 0.7039\n",
      "Epoch 37/100\n",
      "5825/5825 - 1s - loss: 0.5437 - accuracy: 0.7039\n",
      "Epoch 38/100\n",
      "5825/5825 - 1s - loss: 0.5445 - accuracy: 0.7032\n",
      "Epoch 39/100\n",
      "5825/5825 - 1s - loss: 0.5708 - accuracy: 0.6932\n",
      "Epoch 40/100\n",
      "5825/5825 - 1s - loss: 0.5506 - accuracy: 0.6999\n",
      "Epoch 41/100\n",
      "5825/5825 - 1s - loss: 0.5545 - accuracy: 0.6982\n",
      "Epoch 42/100\n",
      "5825/5825 - 1s - loss: 0.5451 - accuracy: 0.7023\n",
      "Epoch 43/100\n",
      "5825/5825 - 1s - loss: 0.5411 - accuracy: 0.7035\n",
      "Epoch 44/100\n",
      "5825/5825 - 1s - loss: 0.5394 - accuracy: 0.7039\n",
      "Epoch 45/100\n",
      "5825/5825 - 1s - loss: 0.5388 - accuracy: 0.7039\n",
      "Epoch 46/100\n",
      "5825/5825 - 1s - loss: 0.5381 - accuracy: 0.7039\n",
      "Epoch 47/100\n",
      "5825/5825 - 1s - loss: 0.5420 - accuracy: 0.7033\n",
      "Epoch 48/100\n",
      "5825/5825 - 1s - loss: 0.5587 - accuracy: 0.7018\n",
      "Epoch 49/100\n",
      "5825/5825 - 1s - loss: 0.5541 - accuracy: 0.7009\n",
      "Epoch 50/100\n",
      "5825/5825 - 1s - loss: 0.5505 - accuracy: 0.7015\n",
      "Epoch 51/100\n",
      "5825/5825 - 1s - loss: 0.5419 - accuracy: 0.7028\n",
      "Epoch 52/100\n",
      "5825/5825 - 1s - loss: 0.5399 - accuracy: 0.7033\n",
      "Epoch 53/100\n",
      "5825/5825 - 1s - loss: 0.5395 - accuracy: 0.7033\n",
      "Epoch 54/100\n",
      "5825/5825 - 1s - loss: 0.5406 - accuracy: 0.7033\n",
      "Epoch 55/100\n",
      "5825/5825 - 1s - loss: 0.5440 - accuracy: 0.7027\n",
      "Epoch 56/100\n",
      "5825/5825 - 1s - loss: 0.5385 - accuracy: 0.7033\n",
      "Epoch 57/100\n",
      "5825/5825 - 1s - loss: 0.5382 - accuracy: 0.7033\n",
      "Epoch 58/100\n",
      "5825/5825 - 1s - loss: 0.5377 - accuracy: 0.7033\n",
      "Epoch 59/100\n",
      "5825/5825 - 1s - loss: 0.5375 - accuracy: 0.7033\n",
      "Epoch 60/100\n",
      "5825/5825 - 1s - loss: 0.5372 - accuracy: 0.7033\n",
      "Epoch 61/100\n",
      "5825/5825 - 1s - loss: 0.5369 - accuracy: 0.7033\n",
      "Epoch 62/100\n",
      "5825/5825 - 1s - loss: 0.5603 - accuracy: 0.6949\n",
      "Epoch 63/100\n",
      "5825/5825 - 1s - loss: 0.5784 - accuracy: 0.6870\n",
      "Epoch 64/100\n",
      "5825/5825 - 1s - loss: 0.5557 - accuracy: 0.6956\n",
      "Epoch 65/100\n",
      "5825/5825 - 1s - loss: 0.5536 - accuracy: 0.6963\n",
      "Epoch 66/100\n",
      "5825/5825 - 1s - loss: 0.5444 - accuracy: 0.6997\n",
      "Epoch 67/100\n",
      "5825/5825 - 1s - loss: 0.5426 - accuracy: 0.7009\n",
      "Epoch 68/100\n",
      "5825/5825 - 1s - loss: 0.5424 - accuracy: 0.7003\n",
      "Epoch 69/100\n",
      "5825/5825 - 1s - loss: 0.5375 - accuracy: 0.7023\n",
      "Epoch 70/100\n",
      "5825/5825 - 1s - loss: 0.5374 - accuracy: 0.7021\n",
      "Epoch 71/100\n",
      "5825/5825 - 1s - loss: 0.5368 - accuracy: 0.7025\n",
      "Epoch 72/100\n",
      "5825/5825 - 1s - loss: 0.5340 - accuracy: 0.7035\n",
      "Epoch 73/100\n",
      "5825/5825 - 1s - loss: 0.5336 - accuracy: 0.7035\n",
      "Epoch 74/100\n",
      "5825/5825 - 1s - loss: 0.5334 - accuracy: 0.7035\n",
      "Epoch 75/100\n",
      "5825/5825 - 1s - loss: 0.5336 - accuracy: 0.7033\n",
      "Epoch 76/100\n",
      "5825/5825 - 1s - loss: 0.5336 - accuracy: 0.7033\n",
      "Epoch 77/100\n",
      "5825/5825 - 1s - loss: 0.5406 - accuracy: 0.7006\n",
      "Epoch 78/100\n",
      "5825/5825 - 1s - loss: 0.5408 - accuracy: 0.7009\n",
      "Epoch 79/100\n",
      "5825/5825 - 1s - loss: 0.5440 - accuracy: 0.6992\n",
      "Epoch 80/100\n",
      "5825/5825 - 1s - loss: 0.5394 - accuracy: 0.7015\n",
      "Epoch 81/100\n",
      "5825/5825 - 1s - loss: 0.5369 - accuracy: 0.7020\n",
      "Epoch 82/100\n",
      "5825/5825 - 1s - loss: 0.5488 - accuracy: 0.7013\n",
      "Epoch 83/100\n",
      "5825/5825 - 1s - loss: 0.5526 - accuracy: 0.6985\n",
      "Epoch 84/100\n",
      "5825/5825 - 1s - loss: 0.5348 - accuracy: 0.7027\n",
      "Epoch 85/100\n",
      "5825/5825 - 1s - loss: 0.5360 - accuracy: 0.7021\n",
      "Epoch 86/100\n",
      "5825/5825 - 1s - loss: 0.5398 - accuracy: 0.7003\n",
      "Epoch 87/100\n",
      "5825/5825 - 1s - loss: 0.5345 - accuracy: 0.7028\n",
      "Epoch 88/100\n",
      "5825/5825 - 1s - loss: 0.5323 - accuracy: 0.7035\n",
      "Epoch 89/100\n",
      "5825/5825 - 1s - loss: 0.5321 - accuracy: 0.7035\n",
      "Epoch 90/100\n",
      "5825/5825 - 1s - loss: 0.5317 - accuracy: 0.7035\n",
      "Epoch 91/100\n",
      "5825/5825 - 1s - loss: 0.5317 - accuracy: 0.7035\n",
      "Epoch 92/100\n",
      "5825/5825 - 1s - loss: 0.5315 - accuracy: 0.7035\n",
      "Epoch 93/100\n",
      "5825/5825 - 1s - loss: 0.5314 - accuracy: 0.7035\n",
      "Epoch 94/100\n",
      "5825/5825 - 1s - loss: 0.5313 - accuracy: 0.7035\n",
      "Epoch 95/100\n",
      "5825/5825 - 1s - loss: 0.5312 - accuracy: 0.7035\n",
      "Epoch 96/100\n",
      "5825/5825 - 1s - loss: 0.5311 - accuracy: 0.7035\n",
      "Epoch 97/100\n",
      "5825/5825 - 1s - loss: 0.5310 - accuracy: 0.7035\n",
      "Epoch 98/100\n",
      "5825/5825 - 1s - loss: 0.5322 - accuracy: 0.7033\n",
      "Epoch 99/100\n",
      "5825/5825 - 1s - loss: 0.5682 - accuracy: 0.6937\n",
      "Epoch 100/100\n",
      "5825/5825 - 1s - loss: 0.5630 - accuracy: 0.6932\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb83a1c4e0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "\n",
    "# Use the training data to fit (train) the model\n",
    "# @NOTE: Experiment with the number of training epochs to find the minimum iterations required to achieve a good accuracy\n",
    "model.fit(X_train_scaled,encoded_y_train,epochs=100,shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1942/1 - 0s - loss: 0.6947 - accuracy: 0.6936\n",
      "Normal Neural Network - Loss: 0.6613665954828508, Accuracy: 0.6936148405075073\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the testing data\n",
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, encoded_y_test, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}